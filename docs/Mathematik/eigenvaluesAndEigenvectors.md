---
title: Eigenvalues And Eigenvectors
tags: [vectors, matrices, linear algebra]
---


<!--- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! Instead, edit the notebook w/the location & name as this file.-->

Before we talk about eigenvalues and eigenvectors let us just remind ourselves that vectors can be transformed using matrices. For example we can rotate a vector using the rotation matrix:

![2dRotationMatrix](/img/maths/2dRotationMatrix.png)

$$
\begin{bmatrix}
  \cos\theta & -\sin\theta \\
  \sin\theta &  \cos\theta \\
\end{bmatrix}\begin{bmatrix}
  x \\
  y \\
\end{bmatrix}
=
\begin{bmatrix}
  x' \\
  y' \\
\end{bmatrix}
$$

Or we can use a matrix to scale a vector:

![scalingVector](/img/maths/scalingVector.png)

$$
\begin{bmatrix}
  10 & 0 \\
  0 & 10 \\
\end{bmatrix}\begin{bmatrix}
  5 \\
  10 \\
\end{bmatrix}
=
\begin{bmatrix}
  50 \\
  100 \\
\end{bmatrix}
$$

Now let us go back to eigenvalues and eigenvectors. An eigenvector $\bm{v}$ of a square matrix $\bm{A}$ is defined as a non-zero vector such that the multiplication with $\bm{A}$ only changes the scale of the vector it does not change the direction. The scalar $\lambda$ is called the eigenvalue.

$$\bm{Av}=\lambda \bm{v}$$

Because there would be an infinite amount of solutions we limit the magnitude of the vector to $\parallel\bm{v}\parallel_2=1$.

Let us look at an example of how to calculate the eigenvector and eigenvalue of

$$
\bm{A}=
\begin{bmatrix}
  0 & 1 \\
  -2 & -3 \\
\end{bmatrix}
$$

For this we can rewrite the problem and solve the following equations:

$$
\begin{align*}
\bm{Av}=\lambda \bm{v} \\
\bm{Av} - \lambda \bm{v} = 0 \\
\bm{Av} - \lambda \bm{Iv} = 0
(\bm{A} - \lambda \bm{I})\bm{v} = 0
\end{align*}
$$

For there to be a solution where $\bm{v}$ is non-zero then the following must be true and which then must lead to the characteristic polynomial of $\bm{A}$. Solving the characteristic polynomial equaling 0 we can get between 0 and $n$ eigenvalues with $n$ being the number of dimensions of $\bm{A} \in \mathbb{R}^{n \times n}$:

$$
\begin{align*}
det(\bm{A}-\lambda\bm{I}) &= 0 \\
det\big(
    \begin{bmatrix}
      0 & 1 \\
      -2 & -3 \\
    \end{bmatrix}
    - \begin{bmatrix}
      \lambda & 0 \\
      0 & \lambda \\
    \end{bmatrix}
\big) &= 0 \\
det\big(
    \begin{bmatrix}
      -\lambda & 1 \\
      -2 & -3-\lambda \\
    \end{bmatrix}
\big) &= \lambda^2+3\lambda+2=0 \\
&\lambda_1 = -1,\,\lambda_2 = -2
\end{align*}
$$

Now that we have the eigenvalues all we need to do is calculate the eigenvectors corresponding to each eigenvalue.

$$
\begin{align*}
(\bm{A} - \lambda \bm{I})\bm{v} &= 0 \\
\big(\begin{bmatrix}
      0 & 1 \\
      -2 & -3 \\
\end{bmatrix}
- \begin{bmatrix}
      -1 & 0 \\
      0 & -1 \\
\end{bmatrix} \big)
\begin{bmatrix}
      v_1 \\
      v_2 \\
\end{bmatrix} &= 0 \\
\begin{bmatrix}
      1 & 1 \\
      -2 & -2 \\
\end{bmatrix}
\begin{bmatrix}
      v_1 \\
      v_2 \\
\end{bmatrix} &= 0 \\
\begin{bmatrix}
      v_1 + v_2 \\
      -2v_1 -2v_2 \\
\end{bmatrix} &= 0 \\
&\Rightarrow v_1 = -v_2
\end{align*}
$$

So we know $v_1 = -v_2$ since we constrict ourselves to vectors with a magnitude of 1 so $\sqrt{v_1^2 + (-v_1)^2}=1$ we get for eigenvalue $\lambda_1=-1$ the eigenvector
$$\bm{v}=\begin{bmatrix}
      0.707107 \\
      -0.707107 \\
\end{bmatrix}$$

We can also calculate this using the following numpy code:


```python
import numpy as np

A = np.array([[0, 1], [-2, -3]])
e_values, e_vectors = np.linalg.eig(A)
print(f"Eigenvalues: {e_values}")
print(f"Eigenvectors: {e_vectors}")
```

<CodeOutputBlock lang="python">

    Eigenvalues: [-1. -2.]
    Eigenvectors: [[ 0.70710678 -0.4472136 ]
     [-0.70710678  0.89442719]]


</CodeOutputBlock>

## Properties

We can use the eigenvalues and eigenvectors of the matrix $\bm{A}$ to find out a lot about it

- The trace of $\bm{A}$ is the sum of its eigenvalues $tr(\bm{A})=\sum_{i=1}^{n}{\lambda_i}$.
- The determinant of $\bm{A}$ is the product of its eigenvalues $tr(\bm{A})=\prod_{i=1}^{n}{\lambda_i}$.
- The rank of $\bm{A}$ is amount of non-zero eigenvalues.



```python
print(f"Trace: {np.trace(A)}")
print(f"Determinant: {np.linalg.det(A)}")
print(f"Rank: {np.linalg.matrix_rank(A)}")
```

<CodeOutputBlock lang="python">

    Trace: -3
    Determinant: 2.0
    Rank: 2


</CodeOutputBlock>

If $\bm{A}$ is a diagonal matrix then the eigenvalues are just the diagonal elements.


```python
D = np.diag([1, 2, 3])
e_values, e_vectors = np.linalg.eig(D)
print(f"Eigenvalues: {e_values}")
print(f"Eigenvectors: {e_vectors}")
```

<CodeOutputBlock lang="python">

    Eigenvalues: [1. 2. 3.]
    Eigenvectors: [[1. 0. 0.]
     [0. 1. 0.]
     [0. 0. 1.]]


</CodeOutputBlock>

## Eigendecomposition

The eigendecomposition is a way to split up **square** matrices into 3 matrices which can be useful in many applications. Eigendecomposition can be pretty easily derived from the above since it lead to the following equations:

$$
\begin{align*}
    \bm{A}= \begin{bmatrix}5 & 2 & 0\\ 2 & 5 & 0\\ 4 & -1 & 4\end{bmatrix} \\
    \bm{A}\begin{bmatrix}1\\ 1\\ 1\end{bmatrix} = 7 \cdot \begin{bmatrix}1\\ 1\\ 1\end{bmatrix} \\
    \bm{A}\begin{bmatrix}0\\ 0\\ 1\end{bmatrix} = 4 \cdot \begin{bmatrix}0\\ 0\\ 1\end{bmatrix} \\
    \bm{A}\begin{bmatrix}-1\\ 1\\ 5\end{bmatrix} = 3 \cdot \begin{bmatrix}-1\\ 1\\ 5\end{bmatrix}
\end{align*}
$$

Instead of holding this information in three separate equations we can combine them to one equation using matrices. We combine the eigenvectors to a matrix where each column is a eigenvector and we create a diagonal matrix with the eigenvalues (by convention in order of small to large):

$$
\begin{align*}
    \bm{A}\begin{bmatrix}
        1 & 0 & -1 \\
        1 & 0 & 1 \\
        1 & 1 & 5
    \end{bmatrix}
    = \begin{bmatrix}
         1 & 0 & -1 \\
        1 & 0 & 1 \\
        1 & 1 & 5
    \end{bmatrix}
    \begin{bmatrix}
        7 & 0 & 0 \\
        0 & 4 & 0 \\
        0 & 0 & 3
    \end{bmatrix}
\end{align*} \\
\bm{AX}=\bm{X}\Lambda \\
\bm{AXX}^{-1}=\bm{X}\Lambda\bm{X}^{-1} \\
\bm{A}=\bm{X}\Lambda\bm{X}^{-1}
$$


```python
A = np.array([[5, 2, 0], [2, 5, 0], [4, -1, 4]])
A
```

<CodeOutputBlock lang="python">




    array([[ 5,  2,  0],
           [ 2,  5,  0],
           [ 4, -1,  4]])



</CodeOutputBlock>


```python
X = np.array([[1, 0, -1], [1, 0, 1], [1, 1, 5]])
Lambda = np.diag([7, 4, 3])
inverse = np.linalg.inv(X)
np.matmul(np.matmul(X, Lambda), inverse)
```

<CodeOutputBlock lang="python">




    array([[ 5.,  2.,  0.],
           [ 2.,  5.,  0.],
           [ 4., -1.,  4.]])



</CodeOutputBlock>

## Singular Value Decomposition - SVD

The eigendecomposition only works for square matrices, the singular value decomposition, short SVD, is a generalization of the eigendecomposition allowing it to be used for rectangular matrices. Singular value decomposition uses 3 matrices just like the eigendecomposition.

$$\bm{A}=\bm{U}\Sigma\bm{V}^T$$

The first matrix $\bm{U}$ is the so-called left singular value matrix which is an orthogonal matrix meaning $\bm{UU}^T=\bm{I}$, the second matrix $\Sigma$ is the singular value matrix which is very just like the matrix containing the eigenvalues in the eigendecomposition a diagonal matrix. The last matrix $\bm{V}^T$ is the right singular value matrix which is also an orthogonal matrix. To find the values we can do the following transformations which make it very similar to the eigendecomposition.

$$
\begin{align*}
    \bm{A}^T\bm{A}=\bm{V}\Sigma^T\bm{U}^T\bm{U}\Sigma\bm{V}^T \\
    \bm{A}^T\bm{A}=\bm{V}(\Sigma^T\Sigma)\bm{V}^T
\end{align*}
$$

Because $\Sigma$ is a diagonal matrix the multiplication with its transpose results again in a diagonal matrix. Which gives it the same form as the eigendecomposition. To get the matrix $\bm{U}$ we can do something very similiar.

$$
\begin{align*}
    \bm{A}\bm{A}^T=\bm{U}\Sigma\bm{V}^T\bm{V}\Sigma^T\bm{U}^T \\
    \bm{A}\bm{A}^T=\bm{U}(\Sigma\Sigma^T)\bm{U}^T
\end{align*}
$$



```python
A = np.array([[-5,2,3], [2, 5, 1], [-3,1,-5]])
e_values, e_vectors = np.linalg.eigh(A.T@A) # @ is same as np.matmul
Sigma = np.diag(np.sqrt(e_values))
V = e_vectors
U = []
for i in range(0, len(e_values)):
    u_i = A@V[:,i]/np.linalg.norm(A@V[:,i])
    U.append(u_i)
U@Sigma@V.T
```

<CodeOutputBlock lang="python">




    array([[-5.18214154,  1.89367286,  2.74943852],
           [ 1.95955405,  5.01675209,  1.2880268 ],
           [-2.7028794 ,  1.11633398, -5.07755598]])



</CodeOutputBlock>

We can see that we lose some precision due to floating number operations but these can be fixed using the round operation.


```python
np.round(U@Sigma@V.T)
```

<CodeOutputBlock lang="python">




    array([[-5.,  2.,  3.],
           [ 2.,  5.,  1.],
           [-3.,  1., -5.]])



</CodeOutputBlock>

We can also just use the built in svd of numpy which is more efficient and accurate.


```python
U, Sigma, Vh = np.linalg.svd(A)
U@np.diag(Sigma)@Vh
```

<CodeOutputBlock lang="python">




    array([[-5.,  2.,  3.],
           [ 2.,  5.,  1.],
           [-3.,  1., -5.]])



</CodeOutputBlock>
