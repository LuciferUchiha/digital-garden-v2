{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e3a58888-3e59-4bfe-b24a-53da37580bea",
   "metadata": {
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "---\n",
    "title: Eigenvalues And Eigenvectors\n",
    "tags: [vectors, matrices, linear algebra]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee2235-a728-4a58-992d-2eda1d0e20bd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we talk about eigenvalues and eigenvectors let us just remind ourselves that vectors can be transformed using matrices. For example we can rotate a vector using the rotation matrix:\n",
    "\n",
    "![2dRotationMatrix](/img/maths/2dRotationMatrix.png)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  \\cos\\theta & -\\sin\\theta \\\\\n",
    "  \\sin\\theta &  \\cos\\theta \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  x \\\\\n",
    "  y \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  x' \\\\\n",
    "  y' \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or we can use a matrix to scale a vector:\n",
    "\n",
    "![scalingVector](/img/maths/scalingVector.png)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  10 & 0 \\\\\n",
    "  0 & 10 \\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "  5 \\\\\n",
    "  10 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "  50 \\\\\n",
    "  100 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now let us go back to eigenvalues and eigenvectors. An eigenvector $\\boldsymbol{v}$ of a square matrix $\\boldsymbol{A}$ is defined as a non-zero vector such that the multiplication with $\\boldsymbol{A}$ only changes the scale of the vector it does not change the direction. The scalar $\\lambda$ is called the eigenvalue.\n",
    "\n",
    "$$\\boldsymbol{Av}=\\lambda \\boldsymbol{v}$$\n",
    "\n",
    "Because there would be an infinite amount of solutions we limit the magnitude of the vector to $\\parallel\\boldsymbol{v}\\parallel_2=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afce862",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us look at an example of how to calculate the eigenvector and eigenvalue of\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}=\n",
    "\\begin{bmatrix}\n",
    "  0 & 1 \\\\\n",
    "  -2 & -3 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For this we can rewrite the problem and solve the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{Av}=\\lambda \\boldsymbol{v} \\\\\n",
    "\\boldsymbol{Av} - \\lambda \\boldsymbol{v} = 0 \\\\\n",
    "\\boldsymbol{Av} - \\lambda \\boldsymbol{Iv} = 0\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For there to be a solution where $\\boldsymbol{v}$ is non-zero then the following must be true and which then must lead to the characteristic polynomial of $\\boldsymbol{A}$. Solving the characteristic polynomial equaling 0 we can get between 0 and $n$ eigenvalues with $n$ being the number of dimensions of $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "det(\\boldsymbol{A}-\\lambda\\boldsymbol{I}) &= 0 \\\\\n",
    "det\\big(\n",
    "    \\begin{bmatrix}\n",
    "      0 & 1 \\\\\n",
    "      -2 & -3 \\\\\n",
    "    \\end{bmatrix}\n",
    "    - \\begin{bmatrix}\n",
    "      \\lambda & 0 \\\\\n",
    "      0 & \\lambda \\\\\n",
    "    \\end{bmatrix}\n",
    "\\big) &= 0 \\\\\n",
    "det\\big(\n",
    "    \\begin{bmatrix}\n",
    "      -\\lambda & 1 \\\\\n",
    "      -2 & -3-\\lambda \\\\\n",
    "    \\end{bmatrix}\n",
    "\\big) &= \\lambda^2+3\\lambda+2=0 \\\\\n",
    "&\\lambda_1 = -1,\\,\\lambda_2 = -2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now that we have the eigenvalues all we need to do is calculate the eigenvectors corresponding to each eigenvalue.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(\\boldsymbol{A} - \\lambda \\boldsymbol{I})\\boldsymbol{v} &= 0 \\\\\n",
    "\\big(\\begin{bmatrix}\n",
    "      0 & 1 \\\\\n",
    "      -2 & -3 \\\\\n",
    "\\end{bmatrix}\n",
    "- \\begin{bmatrix}\n",
    "      -1 & 0 \\\\\n",
    "      0 & -1 \\\\\n",
    "\\end{bmatrix} \\big)\n",
    "\\begin{bmatrix}\n",
    "      v_1 \\\\\n",
    "      v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "      1 & 1 \\\\\n",
    "      -2 & -2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "      v_1 \\\\\n",
    "      v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "\\begin{bmatrix}\n",
    "      v_1 + v_2 \\\\\n",
    "      -2v_1 -2v_2 \\\\\n",
    "\\end{bmatrix} &= 0 \\\\\n",
    "&\\Rightarrow v_1 = -v_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we know $v_1 = -v_2$ since we constrict ourselves to vectors with a magnitude of 1 so $\\sqrt{v_1^2 + (-v_1)^2}=1$ we get for eigenvalue $\\lambda_1=-1$ the eigenvector\n",
    "$$\\boldsymbol{v}=\\begin{bmatrix}\n",
    "      0.707107 \\\\\n",
    "      -0.707107 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We can also calculate this using the following numpy code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a733566-f096-4539-a5a9-5dec8dbd4e76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [-1. -2.]\n",
      "Eigenvectors: [[ 0.70710678 -0.4472136 ]\n",
      " [-0.70710678  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[0, 1], [-2, -3]])\n",
    "e_values, e_vectors = np.linalg.eig(A)\n",
    "print(f\"Eigenvalues: {e_values}\")\n",
    "print(f\"Eigenvectors: {e_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403f470",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Properties\n",
    "\n",
    "We can use the eigenvalues and eigenvectors of the matrix $\\boldsymbol{A}$ to find out a lot about it\n",
    "\n",
    "- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$.\n",
    "- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$.\n",
    "- The rank of $\\boldsymbol{A}$ is amount of non-zero eigenvalues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770a638f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace: -3\n",
      "Determinant: 2.0\n",
      "Rank: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Trace: {np.trace(A)}\")\n",
    "print(f\"Determinant: {np.linalg.det(A)}\")\n",
    "print(f\"Rank: {np.linalg.matrix_rank(A)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d6327",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If $\\boldsymbol{A}$ is a diagonal matrix then the eigenvalues are just the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c46681",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [1. 2. 3.]\n",
      "Eigenvectors: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "D = np.diag([1, 2, 3])\n",
    "e_values, e_vectors = np.linalg.eig(D)\n",
    "print(f\"Eigenvalues: {e_values}\")\n",
    "print(f\"Eigenvectors: {e_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9b3d2",
   "metadata": {},
   "source": [
    "## Trick for $2 \\times 2$ Matrices\n",
    "\n",
    "As presented in this [video by 3Blue1Brown](https://www.youtube.com/watch?v=e50Bj7jn9IQ) there is a cool formula that can be used to calculate the eigenvalues of a $2 \\times 2$ matrix such as $\\boldsymbol{A}=\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix}$. It rests upon two properties that have already been mentioned above:\n",
    "\n",
    "- The trace of $\\boldsymbol{A}$ is the sum of its eigenvalues $tr(\\boldsymbol{A})=\\sum_{i=1}^{n}{\\lambda_i}$. So in other words $a + d = \\lambda_1 + \\lambda_2$. We can also reform this to get the mean value of the two eigenvalues: $\\frac{1}{2}tr(\\boldsymbol{A})=\\frac{a+d}{2}=\\frac{\\lambda_1 + \\lambda_2}{2}=m$\n",
    "- The determinant of $\\boldsymbol{A}$ is the product of its eigenvalues $det(\\boldsymbol{A})=\\prod_{i=1}^{n}{\\lambda_i}$. So in other words $ad - bc = \\lambda_1 \\cdot \\lambda_2 = p$\n",
    "\n",
    "$$\n",
    "\\lambda_1, \\lambda_2 = m \\pm \\sqrt{m^2 - p}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6a400",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "The eigendecomposition is a way to split up **square** matrices into 3 matrices which can be useful in many applications. Eigendecomposition can be pretty easily derived from the above since it lead to the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}= \\begin{bmatrix}5 & 2 & 0\\\\ 2 & 5 & 0\\\\ 4 & -1 & 4\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} = 7 \\cdot \\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} = 4 \\cdot \\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix} \\\\\n",
    "    \\boldsymbol{A}\\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix} = 3 \\cdot \\begin{bmatrix}-1\\\\ 1\\\\ 5\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Instead of holding this information in three separate equations we can combine them to one equation using matrices. We combine the eigenvectors to a matrix where each column is a eigenvector and we create a diagonal matrix with the eigenvalues (by convention in order of small to large):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}\\begin{bmatrix}\n",
    "        1 & 0 & -1 \\\\\n",
    "        1 & 0 & 1 \\\\\n",
    "        1 & 1 & 5\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "         1 & 0 & -1 \\\\\n",
    "        1 & 0 & 1 \\\\\n",
    "        1 & 1 & 5\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        7 & 0 & 0 \\\\\n",
    "        0 & 4 & 0 \\\\\n",
    "        0 & 0 & 3\n",
    "    \\end{bmatrix}\n",
    "\\end{align*} \\\\\n",
    "\\boldsymbol{AX}=\\boldsymbol{X}\\Lambda \\\\\n",
    "\\boldsymbol{AXX}^{-1}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1} \\\\\n",
    "\\boldsymbol{A}=\\boldsymbol{X}\\Lambda\\boldsymbol{X}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66320795",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  2,  0],\n",
       "       [ 2,  5,  0],\n",
       "       [ 4, -1,  4]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[5, 2, 0], [2, 5, 0], [4, -1, 4]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d86fee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  2.,  0.],\n",
       "       [ 2.,  5.,  0.],\n",
       "       [ 4., -1.,  4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1, 0, -1], [1, 0, 1], [1, 1, 5]])\n",
    "Lambda = np.diag([7, 4, 3])\n",
    "inverse = np.linalg.inv(X)\n",
    "np.matmul(np.matmul(X, Lambda), inverse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84d4e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Singular Value Decomposition - SVD\n",
    "\n",
    "The eigendecomposition only works for square matrices, the singular value decomposition, short SVD, is a generalization of the eigendecomposition allowing it to be used for rectangular matrices. Singular value decomposition uses 3 matrices just like the eigendecomposition.\n",
    "\n",
    "$$\\boldsymbol{A}=\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T$$\n",
    "\n",
    "The first matrix $\\boldsymbol{U}$ is the so-called left singular value matrix which is an orthogonal matrix meaning $\\boldsymbol{UU}^T=\\boldsymbol{I}$, the second matrix $\\Sigma$ is the singular value matrix which is very just like the matrix containing the eigenvalues in the eigendecomposition a diagonal matrix. The last matrix $\\boldsymbol{V}^T$ is the right singular value matrix which is also an orthogonal matrix. To find the values we can do the following transformations which make it very similar to the eigendecomposition.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}^T\\boldsymbol{A}=\\boldsymbol{V}\\Sigma^T\\boldsymbol{U}^T\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T \\\\\n",
    "    \\boldsymbol{A}^T\\boldsymbol{A}=\\boldsymbol{V}(\\Sigma^T\\Sigma)\\boldsymbol{V}^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Because $\\Sigma$ is a diagonal matrix the multiplication with its transpose results again in a diagonal matrix. Which gives it the same form as the eigendecomposition. To get the matrix $\\boldsymbol{U}$ we can do something very similiar.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\boldsymbol{A}\\boldsymbol{A}^T=\\boldsymbol{U}\\Sigma\\boldsymbol{V}^T\\boldsymbol{V}\\Sigma^T\\boldsymbol{U}^T \\\\\n",
    "    \\boldsymbol{A}\\boldsymbol{A}^T=\\boldsymbol{U}(\\Sigma\\Sigma^T)\\boldsymbol{U}^T\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40b6b63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.18214154,  1.89367286,  2.74943852],\n",
       "       [ 1.95955405,  5.01675209,  1.2880268 ],\n",
       "       [-2.7028794 ,  1.11633398, -5.07755598]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[-5,2,3], [2, 5, 1], [-3,1,-5]])\n",
    "e_values, e_vectors = np.linalg.eigh(A.T@A) # @ is same as np.matmul\n",
    "Sigma = np.diag(np.sqrt(e_values))\n",
    "V = e_vectors\n",
    "U = []\n",
    "for i in range(0, len(e_values)):\n",
    "    u_i = A@V[:,i]/np.linalg.norm(A@V[:,i])\n",
    "    U.append(u_i)\n",
    "U@Sigma@V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad2e70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that we lose some precision due to floating number operations but these can be fixed using the round operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90c9302",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.,  2.,  3.],\n",
       "       [ 2.,  5.,  1.],\n",
       "       [-3.,  1., -5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(U@Sigma@V.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a4d6a",
   "metadata": {},
   "source": [
    "We can also just use the built in svd of numpy which is more efficient and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dd3406",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.,  2.,  3.],\n",
       "       [ 2.,  5.,  1.],\n",
       "       [-3.,  1., -5.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, Sigma, Vh = np.linalg.svd(A)\n",
    "U@np.diag(Sigma)@Vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ffc4f-ec33-4778-9f85-1b5460a9db9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4874554c910ab0571857260f9b3a2c29827abeee32c252ce8c2203a14b689e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
